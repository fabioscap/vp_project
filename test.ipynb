{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount drive (colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "%cd /content/drive/MyDrive/visiope\n",
    "!rm -rf vp_project\n",
    "!git clone https://github.com/fabioscap/vp_project\n",
    "%cd vp_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load sample photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "p = (0.01,0.05) # remove 95% to 99% of depth values\n",
    "depth_sampling = lambda x: utils.sample_depth_random(x,p)\n",
    "depth_transform = torchvision.transforms.Lambda(depth_sampling) # depth sampling\n",
    "\n",
    "\n",
    "data = utils.NYUDepthV2(\"samples\", \n",
    "                        shape=(240,320),\n",
    "                        depth_transform=depth_transform,\n",
    "                        )\n",
    "\n",
    "import os\n",
    "idxes = [int(file[:-4]) for file in os.listdir(\"samples/RGB\")] # substitute for a data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load models and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "import model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "big_net = model.Net1().to(device)\n",
    "small_net = model.Net3(shape=(16,32,64,128)).to(device)\n",
    "\n",
    "small_net.load_state_dict(torch.load(\"weights/small100.pth\"))\n",
    "big_net.load_state_dict(torch.load(\"weights/big_net_100.pth\"))\n",
    "\n",
    "b_num = sum(p.numel() for p in (small_net).parameters())\n",
    "s_num = sum(p.numel() for p in (big_net).parameters())\n",
    "print(\"big net has {} parameters\".format(b_num))\n",
    "print(\"small net has {} parameters\".format(s_num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10 # n <= 10\n",
    "f, axes = plt.subplots(n,5,figsize=(20,2*n))\n",
    "axes[0][0].set_title(\"RGB\")\n",
    "axes[0][1].set_title(\"Depth sampled\")\n",
    "axes[0][2].set_title(\"big_net\")\n",
    "axes[0][3].set_title(\"small_net\")\n",
    "axes[0][4].set_title(\"Ground Truth\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    small_net.eval()\n",
    "    big_net.eval()\n",
    "    i = 0\n",
    "    for i in range(n):\n",
    "            batch = samples[idxes[i]]\n",
    "            rgb = batch[\"rgb\"].unsqueeze(0).to(device)\n",
    "            depth = batch[\"depth\"].unsqueeze(0).to(device)\n",
    "            depth_t = batch[\"depth_t\"].unsqueeze(0).to(device)\n",
    "            out1 = big_net(rgb,depth_t)\n",
    "            out2 = small_net(rgb,depth_t)\n",
    "            \n",
    "            axes[i][0].imshow(rgb.squeeze().permute(1,2,0).cpu())\n",
    "            axes[i][1].imshow(depth_t.squeeze().cpu(),cmap=\"gray\")\n",
    "            axes[i][2].imshow(out1.squeeze().cpu(),cmap=\"gray\")\n",
    "            axes[i][3].imshow(out2.squeeze().cpu(),cmap=\"gray\")\n",
    "            axes[i][4].imshow(depth.squeeze().cpu(),cmap=\"gray\")\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "import torchvision\n",
    "import model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.transforms import Compose,RandomRotation,RandomHorizontalFlip\n",
    "\n",
    "torch.manual_seed(2806)\n",
    "\n",
    "p = (0.01,0.1) # remove 90% to 99% of depth values\n",
    "depth_sampling = lambda x: utils.sample_depth_random(x,p)\n",
    "\n",
    "common_transform = Compose([RandomHorizontalFlip(),RandomRotation(10)]) # data augmentation\n",
    "depth_transform = torchvision.transforms.Lambda(depth_sampling) # depth sampling\n",
    "\n",
    "data = utils.NYUDepthV2(\"../NYUDepthv2\", \n",
    "                        shape=(240,320),\n",
    "                        common_transform=common_transform,\n",
    "                        depth_transform=depth_transform,\n",
    "                        )\n",
    "\n",
    "train_size = int(0.8*len(data))\n",
    "test_size = len(data)-train_size\n",
    "\n",
    "train_data, test_data = random_split(data,[train_size,test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data,\n",
    "                    batch_size=4,\n",
    "                    shuffle=True,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(test_data,\n",
    "                    batch_size=1,\n",
    "                    shuffle=True,\n",
    "                    num_workers=4,\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[int(torch.rand(1).item()*len(data))]\n",
    "\n",
    "rgb = sample[\"rgb\"]\n",
    "depth = sample[\"depth\"]\n",
    "depth_t = sample[\"depth_t\"]\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,5))\n",
    "\n",
    "\n",
    "ax1.imshow(rgb.permute(1,2,0))\n",
    "ax2.imshow(depth.squeeze(0),cmap=\"gray\")\n",
    "ax3.imshow(depth_t.squeeze(0),cmap=\"gray\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "import model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = model.Net3(shape=(16,32,64,128)).to(device)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr=10**-3)\n",
    "\n",
    "# scheduler = ...\n",
    "\n",
    "num = sum(p.numel() for p in (net).parameters())\n",
    "print(\"parameter number: {}\".format(num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl = torch.tensor([1.,2,1])\n",
    "dif = torch.tensor([1.,0,-1])\n",
    "Sx = torch.einsum(\"i,j->ij\",bl,dif).reshape((1,1,3,3)).to(device) # outer product\n",
    "Sy = torch.einsum(\"i,j->ji\",bl,dif).reshape((1,1,3,3)).to(device)\n",
    "\n",
    "loss = lambda predicted,true: utils.rmse(predicted,true) + 0.1*utils.edge_loss(predicted,true,Sx,Sy)\n",
    "\n",
    "utils.train(model=net,\n",
    "            n_epochs = 30,\n",
    "            loss_fn = loss,\n",
    "            optimizer= optimizer,\n",
    "            device = device,\n",
    "            loader = train_loader,\n",
    "            log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(test_loader))\n",
    "rgb = sample[\"rgb\"].to(device)\n",
    "depth = sample[\"depth\"].to(device)\n",
    "depth_t = sample[\"depth_t\"].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    out = net(rgb,depth_t).squeeze(0)\n",
    "\n",
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20,10))\n",
    "\n",
    "ax1.imshow(rgb[0,...].permute(1,2,0).cpu())\n",
    "ax2.imshow(depth_t[0,...].squeeze().cpu(),cmap=\"gray\")\n",
    "ax3.imshow(depth[0,...].squeeze().cpu(),cmap=\"gray\")\n",
    "ax4.imshow(out.detach()[0,...].squeeze().cpu(),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load/save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),\"weights/small100_.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\"weights/small100.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.compute_accuracy(net,utils.d_accuracy,test_loader,device)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "730cf9b349456fd150aa69f5a53c05307454fca32bda7eea7a960c4826252cc2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('visiope')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
